{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clash of the Soccerbots\n",
    "\n",
    "In this notebook we will control two jetbots to play each other "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the pretrained engine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from jetbot import ObjectDetector\n",
    "\n",
    "model = ObjectDetector('ssd_mobilenet_v2_coco.engine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, the ``ObjectDetector`` class uses the TensorRT Python API to execute the engine that we provide.  It also takes care of preprocessing the input to the neural network, as\n",
    "well as parsing the detected objects.  Right now it will only work for engines created using the ``jetbot.ssd_tensorrt`` package. That package has the utilities for converting\n",
    "the model from the TensorFlow object detection API to an optimized TensorRT engine.\n",
    "\n",
    "Next, let's initialize our camera.  Our detector takes 300x300 pixel input, so we'll set this when creating the camera.\n",
    "\n",
    "> Internally, the Camera class uses GStreamer to take advantage of Jetson Nano's Image Signal Processor (ISP).  This is super fast and offloads\n",
    "> a lot of the resizing computation from the CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Camera\n",
    "\n",
    "camera = Camera.instance(width=300, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's execute our network using some camera input.  By default the ``ObjectDetector`` class expects ``bgr8`` format that the camera produces.  However,\n",
    "you could override the default pre-processing function if your input is in a different format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f9da35b0964e4480c70054636f2b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', format='jpeg', height='300', width='300')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.15496507287025452, -0.2294325903058052)\n",
      "0.7380139708518982\n",
      "0.8619860291481019\n",
      "0.35306932589401774\n",
      "[[{'label': 37, 'confidence': 0.554470419883728, 'bbox': [0.29458367824554443, 0.19462375342845917, 0.39548617601394653, 0.3465110659599304]}]]\n"
     ]
    }
   ],
   "source": [
    "import traitlets\n",
    "import ipywidgets.widgets as widgets\n",
    "from jetbot import bgr8_to_jpeg\n",
    "import cv2\n",
    "\n",
    "image=camera.value\n",
    "\n",
    "image_widget = widgets.Image(format='jpeg', width=300, height=300)\n",
    "width = int(image_widget.width)\n",
    "height = int(image_widget.height)\n",
    "display(image_widget)\n",
    "detections = model(image)\n",
    "\n",
    "for det in detections[0]:\n",
    "    bbox = det['bbox']\n",
    "    cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (255, 0, 0), 2)\n",
    "    center = detection_center(det)\n",
    "    print(center)\n",
    "    print(float(0.8 + 0.4 * center[0]))\n",
    "    print(float(0.8 - 0.4 * center[0]))\n",
    "    print(norm(bbox))\n",
    "          \n",
    "\n",
    "def detection_center(detection):\n",
    "    \"\"\"Computes the center x, y coordinates of the object\"\"\"\n",
    "    bbox = detection['bbox']\n",
    "    center_x = (bbox[0] + bbox[2]) / 2.0 - 0.5\n",
    "    center_y = (bbox[1] + bbox[3]) / 2.0 - 0.5\n",
    "    return (center_x, center_y)\n",
    "    \n",
    "def norm(vec):\n",
    "    \"\"\"Computes the length of the 2D vector\"\"\"\n",
    "    return np.sqrt(vec[0]**2 + vec[1]**2)\n",
    "    \n",
    "image_widget.value = bgr8_to_jpeg(image)\n",
    "\n",
    "print(detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are any COCO objects in the camera's field of view, they should now be stored in the ``detections`` variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control robot to follow central object\n",
    "\n",
    "Now we want our robot to follow an object of the specified class.  To do this we'll do the following\n",
    "\n",
    "1.  Detect objects matching the specified class\n",
    "2.  Select object closest to center of camera's field of vision, this is the 'target' object\n",
    "3.  Steer robot towards target object, otherwise wander\n",
    "4.  If we're blocked by an obstacle, turn left\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "collision_model = torchvision.models.alexnet(pretrained=False)\n",
    "collision_model.classifier[6] = torch.nn.Linear(collision_model.classifier[6].in_features, 5)\n",
    "collision_model.load_state_dict(torch.load('best_model.pth'))\n",
    "device = torch.device('cuda')\n",
    "collision_model = collision_model.to(device)\n",
    "\n",
    "mean = 255.0 * np.array([0.485, 0.456, 0.406])\n",
    "stdev = 255.0 * np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "normalize = torchvision.transforms.Normalize(mean, stdev)\n",
    "\n",
    "def preprocess(camera_value):\n",
    "    global device, normalize\n",
    "    x = camera_value\n",
    "    x = cv2.resize(x, (224, 224))\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    x = torch.from_numpy(x).float()\n",
    "    x = normalize(x)\n",
    "    x = x.to(device)\n",
    "    x = x[None, ...]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now let's initialize our robot so we can control the motors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Robot\n",
    "\n",
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's display all the control widgets and connect the network execution function to the camera updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da215bb665e40b0ab3b4bb7ea57701d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Image(value=b'', format='jpeg', height='300', width='300'), FloatSlider(value=0.â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from jetbot import bgr8_to_jpeg\n",
    "import time\n",
    "\n",
    "turn_left = 1\n",
    "detstatus = 0\n",
    "lookcount = 0\n",
    "blocked_widget = widgets.FloatSlider(min=0.0, max=1.0, value=0.0, description='blocked')\n",
    "red_widget = widgets.FloatSlider(min=0.0, max=1.0, value=0.0, description='red')\n",
    "blue_widget = widgets.FloatSlider(min=0.0, max=1.0, value=0.0, description='blue')\n",
    "green_widget = widgets.FloatSlider(min=0.0, max=1.0, value=0.0, description='green')\n",
    "yellow_widget = widgets.FloatSlider(min=0.0, max=1.0, value=0.0, description='yellow')\n",
    "free_widget = widgets.FloatSlider(min=0.0, max=1.0, value=0.0, description='free')\n",
    "\n",
    "image_widget = widgets.Image(format='jpeg', width=300, height=300)\n",
    "label_widget = widgets.IntText(value=1, description='tracked label')\n",
    "speed_widget = widgets.FloatSlider(value=0.4, min=0.0, max=1.0, description='speed')\n",
    "turn_gain_widget = widgets.FloatSlider(value=0.8, min=0.0, max=2.0, description='turn gain')\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([image_widget, blocked_widget]),\n",
    "    widgets.HBox([red_widget,blue_widget]),\n",
    "    widgets.HBox([green_widget,yellow_widget]),\n",
    "    free_widget,\n",
    "    label_widget,\n",
    "    speed_widget,\n",
    "    turn_gain_widget\n",
    "]))\n",
    "\n",
    "width = int(image_widget.width)\n",
    "height = int(image_widget.height)\n",
    "\n",
    "def detection_center(detection):\n",
    "    \"\"\"Computes the center x, y coordinates of the object\"\"\"\n",
    "    bbox = detection['bbox']\n",
    "    center_x = (bbox[0] + bbox[2]) / 2.0 - 0.1\n",
    "    center_y = (bbox[1] + bbox[3]) / 2.0 - 0.1\n",
    "    return (center_x, center_y)\n",
    "    \n",
    "def norm(vec):\n",
    "    \"\"\"Computes the length of the 2D vector\"\"\"\n",
    "    return np.sqrt(vec[0]**2 + vec[1]**2)\n",
    "\n",
    "def closest_detection(detections):\n",
    "    \"\"\"Finds the detection closest to the image center\"\"\"\n",
    "    closest_detection = None\n",
    "    for det in detections:\n",
    "        center = detection_center(det)\n",
    "        if closest_detection is None:\n",
    "            closest_detection = det\n",
    "        elif norm(detection_center(det)) < norm(detection_center(closest_detection)):\n",
    "            closest_detection = det\n",
    "    return closest_detection\n",
    "        \n",
    "def execute(change):\n",
    "    global turn_left,detstatus,lookcount\n",
    "    image = change['new']\n",
    "    \n",
    "    # execute collision model to determine if blocked\n",
    "    collision_output = collision_model(preprocess(image)).detach().cpu()\n",
    "    y = F.softmax(collision_output.flatten(), dim=0)\n",
    "    y = y.flatten()\n",
    "#     prob_blocked = float(F.softmax(collision_output.flatten(), dim=0)[0])\n",
    "    prob_blocked = float(y[0])\n",
    "    prob_free = float(y[1])\n",
    "    prob_hit = float(y[2])\n",
    "    prob_hitl = float(y[3])\n",
    "    prob_hitr = float(y[4])\n",
    "#     prob_red = float(y[5])\n",
    "#     prob_yellow = float(y[6])\n",
    "    blocked_widget.value = prob_blocked\n",
    "    red_widget.value = prob_hitr\n",
    "    blue_widget.value = prob_hitl\n",
    "    green_widget.value = prob_hit\n",
    "#     yellow_widget.value = prob_yellow\n",
    "    free_widget.value = prob_free\n",
    "    \n",
    "    image_widget.value = bgr8_to_jpeg(image)\n",
    "    \n",
    "    # turn left if blocked\n",
    "    if prob_free > 0.4 and detstatus == 0:\n",
    "        if lookcount < 10:\n",
    "            robot.left(0.2)\n",
    "            time.sleep(0.0025)\n",
    "            lookcount = lookcount + 1\n",
    "        else:\n",
    "            robot.forward(0.3)\n",
    "            lookcount = 0\n",
    "#         return\n",
    "    elif prob_free > 0.4 and detstatus == 1:\n",
    "        robot.forward(0.2)\n",
    "        return\n",
    "    elif prob_blocked > 0.5:\n",
    "#         robot.backward(0.1)\n",
    "#         time.sleep(0.25)\n",
    "        robot.left(0.2)\n",
    "        return\n",
    "    elif prob_hit > 0.5:\n",
    "        print(\"Entered into hit\")\n",
    "        camera.unobserve_all()\n",
    "        time.sleep(1.0)\n",
    "        robot.stop()\n",
    "#         robot.backward(0.2)\n",
    "        time.sleep(0.5)\n",
    "        robot.forward(0.4)\n",
    "        time.sleep(0.5)\n",
    "        robot.stop()\n",
    "        time.sleep(2)\n",
    "        camera.observe(execute, names='value')\n",
    "        print(\"Exiting hit\")\n",
    "        return\n",
    "        \n",
    "    # compute all detected objects\n",
    "    detections = model(image)\n",
    "    \n",
    "    # draw all detections on image\n",
    "    for det in detections[0]:\n",
    "        bbox = det['bbox']\n",
    "        cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (255, 0, 0), 2)\n",
    "    \n",
    "    # select detections that match selected class label\n",
    "    matching_detections = [d for d in detections[0] if d['label'] == 37 or d['label'] == 53 or d['label']== 55 ]\n",
    "    \n",
    "    # get detection closest to center of field of view and draw it\n",
    "    det = closest_detection(matching_detections)\n",
    "    if det is not None:\n",
    "        bbox = det['bbox']\n",
    "        cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (0, 255, 0), 5)\n",
    "    \n",
    "    \n",
    "        \n",
    "    # otherwise go forward if no target detected\n",
    "    if det is None:\n",
    "#         robot.left(0.2)\n",
    "        detstatus = 0\n",
    "        robot.forward(0.2)\n",
    "        \n",
    "    # otherwsie steer towards target\n",
    "    else:\n",
    "        print(\"Entering ball detect\")\n",
    "        detstatus = 1\n",
    "        # move robot forward and steer proportional target's x-distance from center\n",
    "#         time.sleep(0.5)\n",
    "        center = detection_center(det)\n",
    "#         closeval = norm(det)\n",
    "        print(center)\n",
    "#         turn_g = turn_gain_widget.value\n",
    "        turn_g = 0.4\n",
    "        speed_g = 0.8\n",
    "        robot.set_motors(\n",
    "            float(speed_g + turn_g * center[0]),\n",
    "            float(speed_g - turn_g * center[0])\n",
    "        )\n",
    "        time.sleep(1)\n",
    "#         robot.set_motors(0.5,0.5)\n",
    "#         if closeval < 0.4 and closeval > 0.1:\n",
    "#             time.sleep(0.5)\n",
    "#         elif closeval < 0.45 and closeval > 0.40:\n",
    "#             time.sleep(0.7)\n",
    "#         elif closeval > 0.45:\n",
    "#             time.sleep(1)\n",
    "    \n",
    "        camera.unobserve_all()\n",
    "        robot.stop()\n",
    "        \n",
    "        camera.observe(execute, names='value')\n",
    "        print(\"Exiting ball detect\")\n",
    "    \n",
    "    # update image widget\n",
    "    image_widget.value = bgr8_to_jpeg(image)\n",
    "    \n",
    "execute({'new': camera.value})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the block below to connect the execute function to each camera frame update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.unobserve_all()\n",
    "camera.observe(execute, names='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome!  If the robot is not blocked you should see boxes drawn around the detected objects in blue.  The target object (which the robot follows) will be displayed in green.\n",
    "\n",
    "The robot should steer towards the target when it is detected.  If it is blocked by an object it will simply turn left.\n",
    "\n",
    "You can call the code block below to manually disconnect the processing from the camera and stop the robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "camera.unobserve_all()\n",
    "time.sleep(1.0)\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
